{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBKbp_s0Q4hG"
      },
      "outputs": [],
      "source": [
        "-- Retail Sales Insights |SQL(With RFM Analys)\n",
        "-- SECTION 1: INITIAL DATA VALIDATION\n",
        "\n",
        "#1.Checking that all records have been imported successfully.\n",
        "SELECT  COUNT(*)\n",
        "FROM retail_store_sales.retail_store_sales;\n",
        "\n",
        "-- SECTION 2: DATA STAGING SETUP\n",
        "\n",
        "#Rename the original table to create a working staging environment\n",
        "ALTER TABLE retail_store_sales\n",
        "RENAME TO sales_staging_1;\n",
        "\n",
        "#Create a second staging table for data cleaning\n",
        "CREATE TABLE `sales_staging_2` (\n",
        "  `Transaction_ID` text,\n",
        "  `Customer_ID` text,\n",
        "  `Category` text,\n",
        "  `Item_ID` text,\n",
        "  `Price_Per_Unit` text,\n",
        "  `Quantity` text,\n",
        "  `Total_Spent` text,\n",
        "  `Payment_Method` text,\n",
        "  `Location` text,\n",
        "  `Transaction_Date` datetime DEFAULT NULL,\n",
        "  `Discount_Applied` text\n",
        ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n",
        "\n",
        "#Insert data from Stage 1 into Stage 2 for cleaning\n",
        "INSERT INTO sales_staging_2\n",
        "SELECT\t*\n",
        "FROM sales_staging_1;\n",
        "\n",
        "-- SECTION 3: DATA CLEANING AND MISSING VALUE HANDLING\n",
        "\n",
        "#Inspect how missing values appear (they may be represented as '\\N' in Excel)\n",
        "SELECT Item_ID,\n",
        "       CHAR_LENGTH(Item_ID) AS len,\n",
        "       HEX(Item_ID) AS hex\n",
        "FROM sales_staging_2\n",
        "WHERE Item_ID IS NOT NULL\n",
        "LIMIT 50;\n",
        "\n",
        "#Replace '\\N' with actual NULLs\n",
        "UPDATE sales_staging_2\n",
        "SET Item_ID = NULL,\n",
        "\tPrice_Per_Unit=NULL,\n",
        "    QuantiTy=NULL,\n",
        "    Total_Spent=NULL,\n",
        "    Discount_Applied=NULL\n",
        "WHERE Item_ID = '\\\\N'\n",
        "\tOR Price_Per_Unit='\\\\N'\n",
        "    OR Quantity='\\\\N'\n",
        "\tOR Total_Spent='\\\\N'\n",
        "\tOR Discount_Applied='\\\\N';\n",
        "\n",
        "#Check for any records missing Customer_ID\n",
        "SELECT *\n",
        "FROM sales_staging_2\n",
        "WHERE Customer_ID IS NULL;\n",
        "\n",
        "-- SECTION 4: DUPLICATE HANDLING\n",
        "\n",
        "#Removing  Duplicates\n",
        "#This will be straightforward because 'Transaction_ID' acts as a primary key.\n",
        "SELECT Transaction_ID ,COUNT(*) AS trans_ıd_count\n",
        "FROM sales_staging_2\n",
        "GROUP BY Transaction_ID\n",
        "HAVING trans_ıd_count>1;\n",
        "\n",
        "#View duplicate rows for validation before deleting.#\n",
        "SELECT *\n",
        "FROM (\n",
        "SELECT *,\n",
        "ROW_NUMBER() OVER (PARTITION BY Transaction_ID\n",
        "ORDER BY Transaction_ID) AS row_num,\n",
        "COUNT(*) OVER (PARTITION BY Transaction_ID) AS trans_id_count\n",
        "FROM sales_staging_2\n",
        ") duplicates\n",
        "WHERE trans_id_count > 1;\n",
        "\n",
        "#Create Stage 3 table for cleaned and deduplicated data\n",
        "CREATE TABLE `sales_staging_3` (\n",
        "  `Transaction_ID` text,\n",
        "  `Customer_ID` text,\n",
        "  `Category` text,\n",
        "  `Item_ID` text,\n",
        "  `Price_Per_Unit` text,\n",
        "  `Quantity` text,\n",
        "  `Total_Spent` text,\n",
        "  `Payment_Method` text,\n",
        "  `Location` text,\n",
        "  `Transaction_Date` datetime DEFAULT NULL,\n",
        "  `Discount_Applied` text,\n",
        " `row_num` int,\n",
        " `trans_id_count` int\n",
        ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci\n",
        "\n",
        "#Insert deduplicated data\n",
        "INSERT INTO sales_staging_3\n",
        "SELECT *,\n",
        "ROW_NUMBER() OVER (PARTITION BY Transaction_ID\n",
        "ORDER BY Transaction_ID) AS row_num,\n",
        "COUNT(*) OVER (PARTITION BY Transaction_ID) AS trans_id_count\n",
        "FROM sales_staging_2;\n",
        "\n",
        "#Remove duplicate rows\n",
        "DELETE FROM sales_staging_3\n",
        "WHERE row_num>1;\n",
        "\n",
        "#We can drop the helper columns since they’re no longer needed. Removing them will simplify the dataset and help improve the performance of future queries.\n",
        "ALTER TABLE sales_staging_3\n",
        "DROP COLUMN row_num;\n",
        "ALTER TABLE sales_staging_3\n",
        "DROP COLUMN trans_id_count;\n",
        "#Checking the final view of the table\n",
        "SELECT *\n",
        "FROM sales_staging_3;\n",
        "\n",
        "#NOTE:\n",
        "# In this project, I removed all rows containing NULL values in critical columns — specifically, any rows where Item_ID was NULL.\n",
        "#These rows couldn’t be populated with valid data and would have violated data integrity constraints.\n",
        "#However, depending on the situation, deletion isn’t always the best option. If there’s a reliable way to populate missing data, that should be considered first.\n",
        "#Many analysts tend to skip over this decision point too quickly. Generally, there are two main approaches:\n",
        "#a) Delete rows with null values if the data is critical and cannot be recovered.\n",
        "#b) Populate (impute) missing values if they can be calculated or derived logically.\n",
        "#I will also share a separate post on my read.me section to LinkedIn for those interested in diving deeper into this topic.\n",
        "# Thoughtful handling of missing data can significantly improve the quality,reliability and effectivity of any analysis.\n",
        "\n",
        "-- SECTION 5: FINAL DATA STANDARDIZATION\n",
        "\n",
        "#Remove rows with NULL critical values\n",
        "DELETE FROM sales_staging_3\n",
        "WHERE Item_ID IS NULL;\n",
        "\n",
        "#NOTE:\n",
        "#While reviewing the Excel file, I noticed that the “Category” column was not standardized — for example, “Food” was written as “foo” in some cells.\n",
        "#To ensure data consistency and avoid incorrect grouping during exploratory analysis, I standardized these values in Category column by correcting “foo” to “Food.”\n",
        "\n",
        "#Standardize inconsistent category value\n",
        "UPDATE sales_staging_3\n",
        "SET Category='Food'\n",
        "WHERE Category='Foo';\n",
        "\n",
        "SECTION 6: EXPLORATORY DATA ANALYSIS (EDA)\n",
        "\n",
        "\n",
        "#PART 1\n",
        "#Our management team wants to identify which customers are the highest spenders in our stores and determine the product categories on which they spend the most.\n",
        "#Identify top spenders by total spent\n",
        "SELECT Customer_ID,Category,Item_ID,Sum(Total_Spent) AS TOTAL_SPENT\n",
        "FROM sales_staging_3\n",
        "GROUP BY Customer_ID,Category,Item_ID\n",
        "ORDER BY Total_Spent DESC;\n",
        "\n",
        "#NOTE:\n",
        "#Many analysts,management teams,businesses and projects about sales tend to focus on the biggest customers by looking at the highest revenue they’ve generated.\n",
        "#However, a single large order after a long period of inactivity can distort the analysis and may give false results.\n",
        "#Moreover,these types of customers are essentially wild cards—they can make the numbers look impressive but are not always reliable for sustainable growth.\n",
        "#In reality, businesses should pay attention not only to their top spenders but also to the customers who place frequent small or medium-sized orders.\n",
        "#These engaged customers often provide steady revenue, build stronger relationships, and offer more predictable demand over time.\n",
        "#What we have found with SQL is true.However we would like to focus on our frequent customers\n",
        "\n",
        "#Identify frequent and high-value customers\n",
        "WITH Average_Order AS (\n",
        "SELECT Customer_ID,Category,Item_ID,Sum(Total_Spent) AS TOTAL_SPENT,COUNT(Transaction_ID) AS TOTAL_ORDERS,ROUND(SUM(Total_Spent) / COUNT(Transaction_ID), 2) AS AVERAGE_ORDER_VALUE\n",
        "FROM sales_staging_3\n",
        "GROUP BY Customer_ID,Category,Item_ID\n",
        ")\n",
        "SELECT *\n",
        "FROM Average_Order\n",
        "WHERE TOTAL_ORDERS>5\n",
        "ORDER BY AVERAGE_ORDER_VALUE DESC\n",
        "LIMIT 10;\n",
        "\n",
        "#2.After identifying our frequent customers, they want us to determine which products and categories we should prioritize our marketing campaign.\n",
        "\n",
        "SELECT Item_ID,Category,Payment_Method,Location,SUM(TOTAL_SPENT) as total_rev\n",
        "FROM sales_staging_3\n",
        "GROUP BY  Item_ID,Category,Payment_Method,Location\n",
        "ORDER BY total_rev DESC\n",
        "LIMIT 10;\n",
        "\n",
        "Identify top-performing products and categories\n",
        "WITH Product_Rev AS\n",
        "(\n",
        "SELECT Item_ID,Category,Payment_Method,Location,SUM(TOTAL_SPENT) as total_rev\n",
        "FROM sales_staging_3\n",
        "GROUP BY  Item_ID,Category,Payment_Method,Location\n",
        "ORDER BY total_rev DESC\n",
        "LIMIT 50\n",
        ")\n",
        "SELECT Category,COUNT(Category) as frequent_categories,Item_ID,COUNT(Item_ID) AS frequent_ıtems\n",
        "FROM Product_Rev\n",
        "GROUP BY Category,Item_ID\n",
        "ORDER BY frequent_categories DESC\n",
        "LIMIT 10;\n",
        "\n",
        "#3.They also would like to know the best possible time to lauch the campaign on by looking past sale trends.(For better visualization, we focused on sales from the year 2024.)\n",
        "\n",
        "#NOTE:\n",
        "#As many people may guess,many retailers offer deep discounts after the holiday season to decrease their stock.\n",
        "#Therefore,the campaign launch date should probably be after new year in retail businesses.\n",
        "#However, as analysts, we must support our assumptions with factual data.\n",
        "\n",
        "#Analyze monthly sales trends (e.g., for year 2024)\n",
        "SELECT\n",
        "    DATE_FORMAT(Transaction_Date, '%Y-%m') AS month,\n",
        "    SUM(Total_Spent) AS monthly_revenue,\n",
        "    COUNT(*) AS monthly_transactions\n",
        "FROM sales_staging_3\n",
        "WHERE YEAR(Transaction_Date) = 2024\n",
        "GROUP BY DATE_FORMAT(Transaction_Date, '%Y-%m')\n",
        "ORDER BY month;\n",
        "\n",
        "#4.Finally,they would like to know the best possible way to lauch the campaign.Let's take how many transactıon took place in 2024 for decide that.\n",
        "\n",
        "SELECT Location, Count(Transaction_ID)\n",
        "FROM sales_staging_3\n",
        "WHERE Transaction_Date LIKE '%2024%'\n",
        "GROUP BY Location\n",
        "\n",
        "-- SECTION 7: ADVANCED ANALYSIS - RFM SEGMENTATION\n",
        "\n",
        "\n",
        "#To analyze customer engagement, they also require a RFM (Recency, Frequency, Monetary) analysis on this dataset.\n",
        "#After calculating the RFM scores, they want us to classify customers into meaningful segments based on their overall RFM performance.\n",
        "#(Refer to the README file for detailed labeling criteria.)\n",
        "\n",
        "WITH RFM AS\n",
        "(\n",
        "SELECT Customer_ID,count(Transaction_ID)AS Frequency,max(Transaction_Date) as last_purchase,SUM(Total_Spent) AS Monetary,DATEDIFF('2025-01-20',max(Transaction_date)) AS Recency\n",
        "FROM sales_staging_3\n",
        "GROUP BY Customer_ID\n",
        ")\n",
        "SELECT *,\n",
        "CASE\n",
        " WHEN Recency<=2 THEN 5\n",
        " WHEN Recency<=4 THEN 4\n",
        " WHEN Recency<=6 THEN 3\n",
        " WHEN Recency<=8 THEN 2\n",
        "ELSE 1\n",
        "END AS recency_score,\n",
        "CASE\n",
        " WHEN Monetary>=46000 THEN 5\n",
        " WHEN Monetary>=44000 THEN 4\n",
        " WHEN Monetary>=42000 THEN 3\n",
        " WHEN Monetary>=40000 THEN 2\n",
        " ELSE 1\n",
        "END AS monetary_score,\n",
        "CASE\n",
        " WHEN Frequency>=348 THEN 5\n",
        " WHEN Frequency>=336 THEN 4\n",
        " WHEN Frequency>=324 THEN 3\n",
        " WHEN Frequency>=312 THEN 2\n",
        "ELSE 1\n",
        "END AS frequency_score,\n",
        "  (\n",
        "        CASE\n",
        "            WHEN Recency <= 2 THEN 5\n",
        "            WHEN Recency <= 4 THEN 4\n",
        "            WHEN Recency <= 6 THEN 3\n",
        "            WHEN Recency <= 8 THEN 2\n",
        "            ELSE 1\n",
        "        END +\n",
        "        CASE\n",
        "            WHEN Monetary >= 46000 THEN 5\n",
        "            WHEN Monetary >= 44000 THEN 4\n",
        "            WHEN Monetary >= 42000 THEN 3\n",
        "            WHEN Monetary >= 40000 THEN 2\n",
        "            ELSE 1\n",
        "        END +\n",
        "        CASE\n",
        "            WHEN Frequency >= 348 THEN 5\n",
        "            WHEN Frequency >= 336 THEN 4\n",
        "            WHEN Frequency >= 324 THEN 3\n",
        "            WHEN Frequency >= 312 THEN 2\n",
        "            ELSE 1\n",
        "        END\n",
        "    ) AS rfm_score\n",
        "FROM RFM\n",
        "ORDER BY rfm_score DESC\n",
        "LIMIT 10;\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}